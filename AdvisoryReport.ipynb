{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advisory report\n",
    "___\n",
    "<pre>Teamname    : Submission Impossible ðŸ’¥  \n",
    "Group nr    : 32 - Company C  \n",
    "Students    : {Andrei Dragomir, Ece Doganer, MÃ¡rk Kerekes, Ariana Vargas Pastor}  \n",
    "Student nrs : {2669304,         2552855,     2696796,      2710153}  </pre>\n",
    "___\n",
    "\n",
    "#### Structure of the project:\n",
    "1. Data Exploration:\n",
    "- Data visualisation;\n",
    "- Comparisons of our company's hiree distributions as opposed to those of the other companies;\n",
    "- Hiree descriptive data distributions (based on gender, age, nationality and sports) compared to the distributions of all applicants for company C;\n",
    "- Hiree indicative data distributions compared to the distributions of all applicants for company C;\n",
    "- Data processing and cleaning.\n",
    "\n",
    "2. Modelling:\n",
    "- Model M1: Neural Network with single hidden layer and no drop-out\n",
    "\n",
    "- Model M2: Predictive model based on any indicators\n",
    "\n",
    "- Model M3: Model based only on the given descriptors (age, nationality, gender and sports)\n",
    "   \n",
    "    \n",
    "`IMPORTANT DECISIONS: Ratio of training/test data; Model selection; Hyperparameter optimization`\n",
    "    \n",
    "3. Evaluation and advice:\n",
    "- Use _accuracy_ to test predictive models\n",
    "- Analyse one of our models (suggesting M2)\n",
    "    - test different feature combinations that result in the best accuracy rate\n",
    "- Provide advice for the HR department\n",
    "    - Should the model be used?\n",
    "    - How should the model be used?\n",
    "    - What future evaluations and calibrations needed in the future?\n",
    "    - Discuss potential risks imposed by the usage of this model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing the data set and libraries required\n",
    "\n",
    "In terms of data cleaning, we have checked for null values and observed that there are no missing entries after loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "import statistics\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from turtle import title\n",
    "from enum import unique\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Loading dataset and checking for any possible NaN values\n",
    "recruitmentData = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "print(recruitmentData.isnull().values.any())\n",
    "\n",
    "# print(recruitmentData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "### 1.1 Data visualisation of the general population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot combo age for general pop, employees, candidates \n",
    "df = pd.read_csv (r'recruitmentdataset-2022-1.3.csv') \n",
    "\n",
    "#data \n",
    "general_pop_age = df.age \n",
    "candidates_age = df.age[df['company'] == 'C'] \n",
    "employees_age = candidates_age[df['decision'] == True] \n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3) \n",
    "#plot 1 \n",
    "ax1.violinplot(general_pop_age, showmedians=True) \n",
    "ax1.boxplot(general_pop_age) \n",
    "ax1.set_title('Population age') \n",
    "ax1.set_xticks([1]) \n",
    "ax1.set_xticklabels([\"Population\"]) \n",
    "\n",
    "#plot2 \n",
    "ax2.violinplot(candidates_age, showmedians=True) \n",
    "ax2.boxplot(candidates_age) \n",
    "ax2.set_title('Candidates age') \n",
    "ax2.set_xticks([1]) \n",
    "ax2.set_xticklabels([\"Candidates\"]) \n",
    "\n",
    "#plot3 \n",
    "ax3.violinplot(employees_age, showmedians=True) \n",
    "ax3.boxplot(employees_age) \n",
    "ax3.set_title('Employees age') \n",
    "ax3.set_xticks([1]) \n",
    "ax3.set_xticklabels([\"Employees\"]) \n",
    "plt.title('Age comparison') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender comparison\n",
    "df = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "#data \n",
    "pop_male_nr = df['gender'].value_counts()['male']\n",
    "pop_female_nr = df['gender'].value_counts()['female']\n",
    "pop_other_nr = df['gender'].value_counts()['other']\n",
    "\n",
    "candidates = df[df['company'] == 'C']\n",
    "candidates_male = candidates[candidates['gender'] == 'male']\n",
    "candidates_female = candidates[candidates['gender'] == 'female']\n",
    "candidates_other = candidates[candidates['gender'] == 'other']\n",
    "\n",
    "hired = candidates[candidates['decision'] == True]\n",
    "hired_male = hired[hired['gender'] == 'male']\n",
    "hired_female = candidates_female[candidates_female['decision'] == True]\n",
    "hired_other = candidates_other[candidates_other['decision'] == True]\n",
    "\n",
    "print(\"male candidates: \", len(candidates_male))\n",
    "print(\"male employees: \", len(candidates_male[candidates_male['decision'] == True]) )\n",
    "\n",
    "#figure\n",
    "n = 3\n",
    "ind = np.arange(n)\n",
    "width = 0.25\n",
    "x = ['Male', 'Female', 'Other']\n",
    "\n",
    "y_population = [pop_male_nr, pop_female_nr, pop_other_nr]\n",
    "bar1 = plt.bar(ind, y_population, width, color='y')\n",
    "z_candidates = [candidates_male.shape[0], candidates_female.shape[0], candidates_other.shape[0]]\n",
    "bar2 = plt.bar(ind+width, z_candidates, width, color='g')\n",
    "\n",
    "g_hired = [hired_male.shape[0], hired_female.shape[0], hired_other.shape[0]]\n",
    "bar3 = plt.bar(ind+width*2, g_hired, width, color='b')\n",
    "\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Total number\")\n",
    "plt.title(\"Comparison of gender count\")\n",
    "\n",
    "plt.xticks(ind+width, x)\n",
    "plt.legend((bar1,bar2, bar3), ('Population', 'Candidates', 'Employees'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result storage for each model for plotting\n",
    "\n",
    "#nationality comparison\n",
    "df = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "#data \n",
    "pop_dutch = df['nationality'].value_counts()['Dutch']\n",
    "pop_german = df['nationality'].value_counts()['German']\n",
    "pop_belgian = df['nationality'].value_counts()['Belgian']\n",
    "\n",
    "can = df[df['company'] == 'C']\n",
    "can_dutch = can['nationality'].value_counts()['Dutch']\n",
    "can_german = can['nationality'].value_counts()['German']\n",
    "can_belgian = can['nationality'].value_counts()['Belgian']\n",
    "\n",
    "emp = can[can['decision'] == True]\n",
    "emp_dutch = emp['nationality'].value_counts()['Dutch']\n",
    "emp_german = emp['nationality'].value_counts()['German']\n",
    "emp_belgian = emp['nationality'].value_counts()['Belgian']\n",
    "\n",
    "print(can_dutch, emp_dutch, can_german, emp_german, can_belgian, emp_belgian)\n",
    "\n",
    "# figure\n",
    "n =3\n",
    "ind = np.arange(n)\n",
    "width = 0.25\n",
    "x = ['Population', 'Candidates', 'Employees']\n",
    "\n",
    "y_dutch = [pop_dutch, can_dutch, emp_dutch]\n",
    "bar1 = plt.bar(ind, y_population, width, color='y')\n",
    "z_candidates = [can_dutch, can_german, can_belgian]\n",
    "bar2 = plt.bar(ind+width, z_candidates, width, color='g')\n",
    "g_hired = [emp_dutch, emp_german, emp_belgian]\n",
    "bar3 = plt.bar(ind+width*2, g_hired, width, color='b')\n",
    "\n",
    "# plt.xlabel(\"Nationality\")\n",
    "plt.ylabel(\"Total number\")\n",
    "plt.title(\"Comparison of nationality\")\n",
    "\n",
    "plt.xticks(ind+width, ('Dutch', 'German', 'Belgian'))\n",
    "plt.legend((bar1,bar2, bar3), x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hired_dutch = can_dutch - emp_dutch\n",
    "not_hired_german = can_german - emp_german\n",
    "not_hired_belgian = can_belgian - emp_belgian\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(3, 1)\n",
    "\n",
    "labels0 = 'Not hired Dutch', 'Hired Dutch'\n",
    "sizes0 = not_hired_dutch, emp_dutch\n",
    "ax1.pie(sizes0, labels=labels0)\n",
    "\n",
    "labels1 = \"Not hired German\", \"Hired German\"\n",
    "sizes1 = not_hired_german, emp_german\n",
    "ax2.pie(sizes0, labels=labels1)\n",
    "\n",
    "labels2 = \"Not hired Belgian\", \"Hired Belgian\"\n",
    "sizes2 = not_hired_belgian, emp_belgian\n",
    "ax3.pie(sizes2, labels=labels2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 1) \n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "axs = axs.ravel()\n",
    "\n",
    "print(recruitmentData['sport'].unique)\n",
    "for idx, x in enumerate(recruitmentData['sport'].unique()):\n",
    "    emp_sport = len(recruitmentData.query(\"decision and company == 'C' and sport == '%s'\" % x))\n",
    "    can_sport = len(recruitmentData.query(\"company == 'C'\")) - emp_sport\n",
    "    rest_sport = len(recruitmentData) - can_sport\n",
    "\n",
    "    labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply'\n",
    "    ratios = emp_sport, can_sport, rest_sport\n",
    "    axs[idx].pie(ratios, labels=labels)\n",
    "    axs[idx].set_title(x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_debate = len(recruitmentData.query(\"decision and company == 'C' and `ind-debateclub`\"))\n",
    "can_debate = len(recruitmentData.query(\"company == 'C'\")) - emp_debate\n",
    "rest_debate = len(recruitmentData) - can_debate\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_debate, can_debate, rest_debate\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('Debate experience')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "emp_debate = len(recruitmentData.query(\"decision and company == 'C' and not `ind-debateclub`\"))\n",
    "can_debate = len(recruitmentData.query(\"company == 'C'\")) - emp_debate\n",
    "rest_debate = len(recruitmentData) - can_debate\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_debate, can_debate, rest_debate\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('No debate experience')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of professional experience\n",
    "df = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "#data \n",
    "pop_int_y = df['ind-international_exp'].value_counts()[True]\n",
    "pop_int_n = df['ind-international_exp'].value_counts()[False]\n",
    "\n",
    "can = df[df['company'] == 'C']\n",
    "can_int_y = can['ind-international_exp'].value_counts()[True]\n",
    "can_int_n = can['ind-international_exp'].value_counts()[False]\n",
    "\n",
    "emp = can[can['decision'] == True]\n",
    "emp_int_y = emp['ind-international_exp'].value_counts()[True]\n",
    "emp_int_n = emp['ind-international_exp'].value_counts()[False]\n",
    "\n",
    "#fig\n",
    "n = 3\n",
    "ind = np.arange(n)\n",
    "width = 0.20\n",
    "x = ['Population', 'Candidates', 'Employees']\n",
    "y = ['professional international experience', 'no professional international experience']\n",
    "\n",
    "has_exp = [pop_int_y, can_int_y, emp_int_y]\n",
    "bar1 = plt.bar(ind, has_exp, width, color='y')\n",
    "no_exp = [pop_int_n, can_int_n, emp_int_n]\n",
    "bar2 = plt.bar(ind+width, no_exp, width, color='g')\n",
    "\n",
    "plt.ylabel(\"Total number of people\")\n",
    "plt.title(\"Comparison of professional international experience\")\n",
    "\n",
    "plt.xticks(ind+width, x)\n",
    "plt.legend((bar1,bar2), y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Result storage for each model for plotting\n",
    "\n",
    "# #languages comparison\n",
    "df = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "# #data \n",
    "pop_lang1 = df['ind-languages'].value_counts()[0]\n",
    "pop_lang2 = df['ind-languages'].value_counts()[1]\n",
    "pop_lang3 = df['ind-languages'].value_counts()[2]\n",
    "pop_lang4 = df['ind-languages'].value_counts()[3]\n",
    "\n",
    "can = df[df['company'] == 'C']\n",
    "can_lang1 = can['ind-languages'].value_counts()[0]\n",
    "can_lang2 = can['ind-languages'].value_counts()[1]\n",
    "can_lang3 = can['ind-languages'].value_counts()[2]\n",
    "can_lang4 = can['ind-languages'].value_counts()[3]\n",
    "\n",
    "emp = can[can['decision'] == True]\n",
    "emp_lang1 = emp['ind-languages'].value_counts()[0]\n",
    "emp_lang2 = emp['ind-languages'].value_counts()[1]\n",
    "emp_lang3 = emp['ind-languages'].value_counts()[2]\n",
    "emp_lang4 = emp['ind-languages'].value_counts()[3]\n",
    "\n",
    "# figure\n",
    "n = 3\n",
    "ind = np.arange(n)\n",
    "width = 0.20\n",
    "x = ['Population', 'Candidates', 'Employees']\n",
    "y = ['0 additional languages', '1 additional language', '2 additional languages', '3 additional languages']\n",
    "\n",
    "lang1 = [pop_lang1, can_lang1, emp_lang1]\n",
    "bar1 = plt.bar(ind, lang1, width, color='y')\n",
    "lang2 = [pop_lang2,can_lang2,emp_lang2]\n",
    "bar2 = plt.bar(ind+width, lang2, width, color='g')\n",
    "lang3 = [pop_lang3, can_lang3, emp_lang3]\n",
    "bar3 = plt.bar(ind+width*2, lang3, width, color='b')\n",
    "lang4 = [pop_lang4, can_lang4, emp_lang4]\n",
    "bar4 = plt.bar(ind+width*3, lang4, width, color='r')\n",
    "\n",
    "plt.ylabel(\"Total number of people\")\n",
    "plt.title(\"Comparison of amount of additional languages known\")\n",
    "\n",
    "plt.xticks(ind+width, x)\n",
    "plt.legend((bar1,bar2, bar3, bar4), y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitmentData = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "emp_sport = len(recruitmentData.query(\"decision and company == 'C' and `ind-exact_study`\"))\n",
    "can_sport = len(recruitmentData.query(\"company == 'C'\")) - emp_sport\n",
    "rest_sport = len(recruitmentData) - can_sport\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_sport, can_sport, rest_sport\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('Scientific background')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "emp_sport = len(recruitmentData.query(\"decision and company == 'C' and not `ind-exact_study`\"))\n",
    "can_sport = len(recruitmentData.query(\"company == 'C'\")) - emp_sport\n",
    "rest_sport = len(recruitmentData) - can_sport\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_sport, can_sport, rest_sport\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('No scientific background')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitmentData = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "emp = len(recruitmentData.query(\"decision and company == 'C' and `ind-degree` == 'bachelor'\"))\n",
    "can = len(recruitmentData.query(\"company == 'C'\")) - emp\n",
    "rest = len(recruitmentData) - can\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp, can, rest\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('Bachelor background')\n",
    "\n",
    "plt.show() \n",
    "emp = len(recruitmentData.query(\"decision and company == 'C' and `ind-degree` == 'phd'\"))\n",
    "can = len(recruitmentData.query(\"company == 'C'\")) - emp\n",
    "rest = len(recruitmentData) - can\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp, can, rest\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('PhD background')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "emp = len(recruitmentData.query(\"decision and company == 'C' and `ind-degree` == 'master'\"))\n",
    "can = len(recruitmentData.query(\"company == 'C'\")) - emp\n",
    "rest = len(recruitmentData) - can\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp, can, rest\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('Master background')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitmentData = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "emp_prog = len(recruitmentData.query(\"decision and company == 'C' and `ind-programming_exp`\"))\n",
    "can_prog = len(recruitmentData.query(\"company == 'C'\")) - emp_prog\n",
    "rest_prog = len(recruitmentData) - can_prog\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_prog, can_prog, rest_prog\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('Programming experience')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "emp_prog = len(recruitmentData.query(\"decision and company == 'C' and not `ind-programming_exp`\"))\n",
    "can_prog = len(recruitmentData.query(\"company == 'C'\")) - emp_prog\n",
    "rest_prog = len(recruitmentData) - can_prog\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_prog, can_prog, rest_prog\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('No programming experience')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitmentData = pd.read_csv (r'recruitmentdataset-2022-1.3.csv')\n",
    "\n",
    "emp_entr = len(recruitmentData.query(\"decision and company == 'C' and `ind-entrepeneur_exp`\"))\n",
    "can_entr = len(recruitmentData.query(\"company == 'C'\")) - emp_entr\n",
    "rest_entr = len(recruitmentData) - can_entr\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_entr, can_entr, rest_entr\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('Entrepreneur experience')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "emp_entr = len(recruitmentData.query(\"decision and company == 'C' and not `ind-entrepeneur_exp`\"))\n",
    "can_entr = len(recruitmentData.query(\"company == 'C'\")) - emp_entr\n",
    "rest_entr = len(recruitmentData) - can_entr\n",
    "\n",
    "labels = 'hired candidates', 'not hired candidates', 'population who didn`t apply' \n",
    "ratios = emp_entr, can_entr, rest_entr\n",
    "plt.pie(ratios, labels=labels)\n",
    "plt.title('No entrepreneur experience')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data cleaning and preparation as well as evaluating highest correlating parameters\n",
    "\n",
    "In this section we will evaluate the features that we have visualized in the plotting above, make some assumptions and test them in terms of data meaningfulness.\n",
    "These assumption will be used when building a model in the hopes of achieving a fair discrete alternative to our categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus the dataset to our company\n",
    "dataSet = recruitmentData.query(\"company == 'C'\")\n",
    "dataSetC = pd.DataFrame(dataSet, columns=[\n",
    "    'age',\n",
    "    'gender',\n",
    "    'nationality',\n",
    "    'sport',\n",
    "    'ind-university_grade',\n",
    "    'ind-debateclub',\n",
    "    'ind-programming_exp',\n",
    "    'ind-international_exp',\n",
    "    'ind-entrepeneur_exp',\n",
    "    'ind-languages',\n",
    "    'ind-exact_study',\n",
    "    'ind-degree',\n",
    "    'decision'\n",
    "])\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "\n",
    "# Convert the following numerical labels from interger to float\n",
    "conversions = {\n",
    "    'ind-languages' : float,\n",
    "    'ind-university_grade' : float\n",
    "}\n",
    "dataSetC = dataSetC.astype(conversions)\n",
    "\n",
    "#Transoform age into age groups depending on the general distribution of ages in the dataset\n",
    "bins= recruitmentData['age'].describe()[3:8]\n",
    "labels = ['21-24','24-26','26-28','28-32']\n",
    "dataSetC['ageGroup'] = pd.cut(dataSetC['age'], bins=bins, labels=labels, right=False)\n",
    "dataSetC = dataSetC.drop('age', axis=1)\n",
    "dataSetC = dataSetC.dropna()\n",
    "    \n",
    "# Label Encoder conversion\n",
    "dataSetC['decision'] = labelEncoder.fit_transform(dataSetC['decision'])\n",
    "dataSetC['ind-debateclub'] = labelEncoder.fit_transform(dataSetC['ind-debateclub'])\n",
    "dataSetC['ind-entrepeneur_exp'] = labelEncoder.fit_transform(dataSetC['ind-entrepeneur_exp'])\n",
    "dataSetC['ind-exact_study'] = labelEncoder.fit_transform(dataSetC['ind-exact_study'])\n",
    "dataSetC['ind-programming_exp'] = labelEncoder.fit_transform(dataSetC['ind-programming_exp'])\n",
    "dataSetC['ind-international_exp'] = labelEncoder.fit_transform(dataSetC['ind-international_exp'])    \n",
    "\n",
    "# One Hot Encoding conversion for gender, sport, agegroups and degree\n",
    "dataSetC = pd.get_dummies(dataSetC)\n",
    "\n",
    "tmp = dataSetC['decision']\n",
    "# Scale our data \n",
    "scaler = StandardScaler()\n",
    "dataSetCNew = pd.DataFrame(scaler.fit_transform(dataSetC), columns= dataSetC.columns)\n",
    "dataSetCNew['decision'] = tmp.values\n",
    "\n",
    "print(dataSetCNew)\n",
    "# Evaluating correlations in order to potentially find good combinations of features\n",
    "corr = dataSetCNew.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "seaborn.heatmap(corr, mask=np.zeros_like(corr), cmap=seaborn.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True)\n",
    "plt.show()            \n",
    "ranking = corr['decision']\n",
    "ranking = ranking.sort_values()\n",
    "ranking.name = \"Ranking of the predictive power of indicators\"\n",
    "print(ranking)\n",
    "\n",
    "print(dataSetCNew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result storage for each model for plotting\n",
    "modelResults = pd.DataFrame(columns=['model','fit_time','score_time','test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model 1: Neural Network with single hidden layer and no drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import get_scorer_names\n",
    "\n",
    "results = pd.DataFrame(columns=['model','fit_time','score_time','test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'features'])\n",
    "models = []\n",
    "inputChoices = [pd.DataFrame(dataSetCNew, columns=['ind-languages', 'ind-degree_phd', 'ind-degree_master', 'ind-international_exp']),\n",
    "\t\t\t\tdataSetCNew.filter(like='ind').sample(n=4,axis='columns'),\n",
    "\t\t\t\tdataSetCNew.filter(like='ind').sample(n=3,axis='columns'),\n",
    "\t\t\t\tdataSetCNew.filter(like='ind').sample(n=5,axis='columns')]\n",
    "target = dataSetC['decision']\n",
    "\n",
    "#Setting hyperparameters and evaluation outputs\n",
    "seed = 7\n",
    "splits = 10\n",
    "epochs = 50\n",
    "scores = ['accuracy', 'precision_micro', 'recall', 'f1']\n",
    "\n",
    "# Clear epoch data for new run if it's been ran before\n",
    "f = open(\"epochAnalysisM1.csv\", \"w\")\n",
    "f.truncate()\n",
    "f.close()\n",
    "\n",
    "#Training model for each input variation\n",
    "for idx, input in enumerate(inputChoices):\t\n",
    "\n",
    "\tkfold = model_selection.KFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tdataLogger = CSVLogger('epochAnalysisM1.csv', separator=\",\", append=True)\n",
    "\t\n",
    "\t#input layer\n",
    "\tmodel.add(Dense(6, kernel_initializer='uniform', activation = 'relu', input_dim = input.columns.size))\n",
    "\n",
    "\t#output layer\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "\t#run model M1\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tkeras_clf = KerasClassifier(model = model, optimizer=\"adam\", epochs=epochs, verbose=0, callbacks=[dataLogger])\n",
    "\n",
    "\tcv_results = model_selection.cross_validate(keras_clf, input, target, cv=kfold, scoring=scores)\n",
    "\t\n",
    "\n",
    "\t#Saving dataFrame of epoch based results for plotting\n",
    "\tnew_row = {'model': 'M1', \n",
    "\t\t\t'fit_time' : cv_results['fit_time'].mean(),\n",
    "\t\t\t'score_time' : cv_results['score_time'].mean(),\n",
    "\t\t\t'test_accuracy' : cv_results['test_accuracy'].mean(),\n",
    "\t\t\t'test_precision' : cv_results['test_precision_micro'].mean(),\n",
    "\t\t\t'test_recall' : cv_results['test_recall'].mean(),\n",
    "\t\t\t'test_f1' : cv_results['test_f1'].mean(),\n",
    "\t\t\t'features' : ', '.join(input.columns)}\n",
    "\tresults = results.append(new_row, ignore_index=True)\n",
    "\n",
    "modelResults = modelResults.append(results.iloc[0].transpose(), ignore_index=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up figure for result plotting\n",
    "fig, axs = plt.subplots(2,2)\n",
    "axs = axs.ravel()\n",
    "fig.suptitle('Training metric changes (over epochs) of M1 based on different feature choices')\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "\n",
    "#Plotting epoch accuracy and loss\n",
    "epochData = pd.read_csv (r'epochAnalysisM1.csv')\n",
    "sectionStart = 0\n",
    "\n",
    "for index in range(0, len(inputChoices), 1):\n",
    "    #Select the logs only of one set of inputs\n",
    "    sectionOfInterest = epochData.loc[(index * epochs * splits):((index + 1) * epochs * splits)]\n",
    "\n",
    "    #Prepare container for data compression\n",
    "    averagedFrame = pd.DataFrame(columns=epochData.columns)\n",
    "    for idx, epoch in enumerate(sectionOfInterest['epoch'].unique()):\n",
    "        oneEpoch = sectionOfInterest.query('epoch == %i' % epoch)\n",
    "        averagedFrame = averagedFrame.append({'epoch' : epoch,\n",
    "                                            'accuracy' : oneEpoch['accuracy'].mean(),\n",
    "                                            'loss' : oneEpoch['loss'].mean()\n",
    "                                            }, ignore_index=True)\n",
    "    \n",
    "    axs[index].set_title('Feature set: %s' % ', '.join(inputChoices[index].columns))\n",
    "    axs[index].plot(averagedFrame['accuracy'], label='M1 feature set %i accuracy' % (index + 1))\n",
    "    axs[index].plot(averagedFrame['loss'], label='M1 feature set %i loss' % (index + 1))\n",
    "    axs[index].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Plotting results from each model input\n",
    "interestData = results.drop(['model', 'fit_time', 'score_time', 'features'], axis=1)\n",
    "XResultsAxis = np.arange(len(interestData.columns))\n",
    "interestData = interestData.transpose()\n",
    "\n",
    "fig, axs = plt.subplots(1,1)\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "for idx, x in enumerate(results['model']):\n",
    "    axis = axs.bar(XResultsAxis + idx/(len(interestData.columns) * 2), interestData[idx].values, width= 1/(len(interestData.columns) * 2), label='Feature set %i' % (idx + 1))\n",
    "    \n",
    "axs.set_xticks(XResultsAxis)\n",
    "axs.set_xticklabels(results.drop(['model', 'fit_time', 'score_time', 'features'], axis=1).columns)\n",
    "axs.set_xlabel('Metrics')\n",
    "axs.set_ylabel('Results')\n",
    "axs.set_title('Metric result comparison for M1 with different feature sets')\n",
    "axs.legend()\n",
    "plt.show()\n",
    "\n",
    "print(results['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model 2: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_model = [ 'ind-degree_master', 'ind-international_exp',\n",
    "                      'ind-exact_study', 'ind-debateclub' , 'ind-languages' ]\n",
    "dataInScope = pd.DataFrame(dataSetCNew, columns=features_to_model)\n",
    "\n",
    "x = pd.DataFrame(dataInScope.values)\n",
    "y = dataSetC['decision'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "decision_tree = clf.fit(x_train, y_train)\n",
    "\n",
    "#visualize tree\n",
    "plt.subplots(figsize=(30, 20))\n",
    "tree.plot_tree(decision_tree, filled=True, rounded=True)\n",
    "plt.savefig(\"decision_tree.png\")\n",
    "\n",
    "#importance of feaures\n",
    "importances = clf.feature_importances_\n",
    "forest_importances = pd.Series(importances, index = features_to_model)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar()\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "\n",
    "#accuracy and precision\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Model accuracy score : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "precision_recall = precision_recall_fscore_support(y_pred, y_test, average= 'macro')\n",
    "print('Precision score (weighted):', precision_recall[0])\n",
    "print('Recall score (weighted):', precision_recall[1])\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print('F1 score (weighted):', f1)\n",
    "\n",
    "#Saving dataFrame of results for plotting\n",
    "new_row = {'model': 'M2', \n",
    "\t\t\t'fit_time' : 0,\n",
    "\t\t\t'score_time' : 0,\n",
    "\t\t\t'test_accuracy' : accuracy_score(y_test, y_pred),\n",
    "\t\t\t'test_precision' : precision_recall[0],\n",
    "\t\t\t'test_recall' : precision_recall[1],\n",
    "\t\t\t'test_f1' : f1,\n",
    "\t\t\t'features' : ', '.join(features_to_model)}\n",
    "\t\n",
    "\n",
    "modelResults = modelResults.append(new_row, ignore_index=True)\n",
    "print(modelResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model 3: Neural Network with single hidden layer and no drop-out on descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "results = pd.DataFrame(columns=['model','fit_time','score_time','test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'features'])\n",
    "models = []\n",
    "\n",
    "\n",
    "\n",
    "inputChoices = [pd.DataFrame(dataSetCNew, columns=['sport_Tennis', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Running', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Swimming', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Chess',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Football',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Golf',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Cricket',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Rugby',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'gender_female',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'gender_other',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'gender_male',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'nationality_German',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'nationality_Dutch',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'nationality_Belgian',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_21-24',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_24-26',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_26-28',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_28-32'])\n",
    "\t\t\t\t]\n",
    "\n",
    "target = dataSetC['decision']\n",
    "\n",
    "#Setting hyperparameters and evaluation outputs\n",
    "seed = 7\n",
    "splits = 10\n",
    "epochs = 50\n",
    "scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Clear epoch data for new run if it's been ran before\n",
    "f = open(\"epochAnalysisM2.csv\", \"w\")\n",
    "f.truncate()\n",
    "f.close()\n",
    "\n",
    "#Training model for each input variation\n",
    "for idx, input in enumerate(inputChoices):\t\n",
    "\n",
    "\tkfold = model_selection.KFold(n_splits=splits, random_state=seed, shuffle=True)\n",
    "\tmodel = Sequential()\n",
    "\tdataLogger = CSVLogger('epochAnalysisM2.csv', separator=\",\", append=True)\n",
    "\t#input layer\n",
    "\tmodel.add(Dense(6, kernel_initializer='uniform', activation = 'relu', input_dim = input.columns.size))\n",
    "\n",
    "\t#output layer\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "\n",
    "\t#run model M1\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'],run_eagerly=True)\n",
    "\tkeras_clf = KerasClassifier(model = model, optimizer=\"adam\", epochs=epochs, verbose=0, callbacks=[dataLogger])\n",
    "\tcv_results = model_selection.cross_validate(keras_clf, input, target, cv=kfold, scoring=scores)\n",
    "\n",
    "\t#Saving dataFrame of epoch based results for plotting\n",
    "\tnew_row = {'model': 'M3', \n",
    "\t\t\t'fit_time' : cv_results['fit_time'].mean(),\n",
    "\t\t\t'score_time' : cv_results['score_time'].mean(),\n",
    "\t\t\t'test_accuracy' : cv_results['test_accuracy'].mean(),\n",
    "\t\t\t'test_precision' : cv_results['test_precision'].mean(),\n",
    "\t\t\t'test_recall' : cv_results['test_recall'].mean(),\n",
    "\t\t\t'test_f1' : cv_results['test_f1'].mean(),\n",
    "\t\t\t'features' : ', '.join(input.columns)}\n",
    "\tresults = results.append(new_row, ignore_index=True)\n",
    "\n",
    "modelResults = modelResults.append(results, ignore_index=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting of Model 3 metrics using all descriptive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interestData = results.drop(['model', 'fit_time', 'score_time', 'features'], axis=1)\n",
    "XResultsAxis = np.arange(len(interestData.columns))\n",
    "interestData = interestData.transpose()\n",
    "\n",
    "fig, axs = plt.subplots(1,1)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "axis = axs.bar(XResultsAxis + idx/(len(interestData.columns) * 2), interestData[idx].values, width= 1/(len(interestData.columns) * 2), label='Model 3')\n",
    "axs.set_xticks(XResultsAxis)\n",
    "axs.set_xticklabels(results.drop(['model', 'fit_time', 'score_time', 'features'], axis=1).columns)\n",
    "axs.set_xlabel('Metrics')\n",
    "axs.set_ylabel('Results')\n",
    "axs.set_title('Metric result comparison for M1 with different feature sets')\n",
    "axs.legend()\n",
    "plt.show()\n",
    "\n",
    "print(results['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting of Model 3 metrics for different subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#Options for each descriptor\n",
    "ageColumns = ['ageGroup_21-24', 'ageGroup_24-26', 'ageGroup_26-28', 'ageGroup_28-32']\n",
    "nationalityColumns = ['nationality_German', 'nationality_Dutch', 'nationality_Belgian']\n",
    "genderColumns = ['gender_female', 'gender_other', 'gender_male']\n",
    "\n",
    "metrics = pd.DataFrame(columns=['model', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'features'])\n",
    "\n",
    "dataSet = pd.DataFrame(dataSetCNew, columns=['sport_Tennis', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Running', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Swimming', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Chess',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Football',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Golf',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Cricket',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'sport_Rugby',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'gender_female',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'gender_other',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'gender_male',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'nationality_German',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'nationality_Dutch',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'nationality_Belgian',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_21-24',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_24-26',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_26-28',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'ageGroup_28-32'])\n",
    "\t\t\t\t\n",
    "#Selecting a random combination of 3 descriptive features, 12 times\n",
    "x = 0\n",
    "while x < 12:\n",
    "\tchoices = [random.choice(ageColumns), random.choice(nationalityColumns), random.choice(genderColumns)]\n",
    "\tquery = \"`%s` > 0 and `%s` > 0 and `%s` > 0\" % (choices[0], choices[1], choices[2])\n",
    "\ttmpInput = dataSet.query(query)\n",
    "\tprint(len(tmpInput))\n",
    "\t#If the queried combination has no inputs, we skip it\n",
    "\tif len(tmpInput) != 0:\n",
    "\t\tx_pred = tmpInput\n",
    "\t\ty_test = dataSetCNew.query(query)['decision']\n",
    "\t\ty_pred = model.predict(x_pred)\n",
    "\n",
    "\t\t#Take all scores from the prediction\n",
    "\t\tnew_row = {'model': 'M3', \n",
    "\t\t\t\t'test_accuracy' : accuracy_score(y_test.values,list(map(lambda x: 0 if x<0.5 else 1, y_pred))),\n",
    "\t\t\t\t'test_precision' : precision_score(y_test.values,list(map(lambda x: 0 if x<0.5 else 1, y_pred))),\n",
    "\t\t\t\t'test_recall' : recall_score(y_test.values,list(map(lambda x: 0 if x<0.5 else 1, y_pred))),\n",
    "\t\t\t\t'test_f1' : f1_score(y_test.values,list(map(lambda x: 0 if x<0.5 else 1, y_pred))),\n",
    "\t\t\t\t'features' : ', '.join(choices)}\n",
    "\t\tmetrics = metrics.append(new_row, ignore_index=True)\n",
    "\t\tx += 1\n",
    "\n",
    "#Plotting results from each model input\n",
    "interestData = metrics.drop(['model', 'features'], axis=1)\n",
    "XResultsAxis = np.arange(len(interestData.columns))\n",
    "interestData = interestData.transpose()\n",
    "\n",
    "fig, axs = plt.subplots(1,1)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "for idx, x in enumerate(metrics['model']):\n",
    "    axis = axs.bar(XResultsAxis + idx/(len(interestData.columns) * 2), interestData[idx].values, width= 1/(len(interestData.columns) * 2), label=metrics['features'][idx])\n",
    "    \n",
    "axs.set_xticks(XResultsAxis)\n",
    "axs.set_xticklabels(results.drop(['model', 'fit_time', 'score_time', 'features'], axis=1).columns)\n",
    "axs.set_xlabel('Metrics')\n",
    "axs.set_ylabel('Results')\n",
    "axs.set_title('Metric result comparison for all models')\n",
    "axs.legend()\n",
    "plt.show()\n",
    "\n",
    "# print(metrics)\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting results from each model input\n",
    "interestData = modelResults.drop(['model', 'fit_time', 'score_time', 'features'], axis=1)\n",
    "XResultsAxis = np.arange(len(interestData.columns))\n",
    "interestData = interestData.transpose()\n",
    "\n",
    "fig, axs = plt.subplots(1,1)\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "for idx, x in enumerate(modelResults['model']):\n",
    "    axis = axs.bar(XResultsAxis + idx/(len(interestData.columns) * 2), interestData[idx].values, width= 1/(len(interestData.columns) * 2), label='Model %i' % (idx + 1))\n",
    "    \n",
    "axs.set_xticks(XResultsAxis)\n",
    "axs.set_xticklabels(results.drop(['model', 'fit_time', 'score_time', 'features'], axis=1).columns)\n",
    "axs.set_xlabel('Metrics')\n",
    "axs.set_ylabel('Results')\n",
    "axs.set_title('Metric result comparison for all models')\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da9fddbb10c13d74a17e4662a940d3abf594ba4b4ea9e3627d1a27856bb85182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
